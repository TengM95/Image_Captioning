{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "from solver import *\n",
    "import sys\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. create input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gzip\n",
    "# import shutil\n",
    "# with gzip.open('dataset_coco.json.gz', 'rb') as f_in:\n",
    "#     with open('dataset_coco.json', 'wb') as f_out:\n",
    "#         shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import create_input_files\n",
    "\n",
    "\n",
    "# # Create input files (along with word map)\n",
    "# create_input_files(dataset='coco',\n",
    "#                    karpathy_json_path='./dataset_coco.json',\n",
    "#                    image_folder='/datasets/COCO-2015/',\n",
    "#                    captions_per_image=5,\n",
    "#                    min_word_freq=5,\n",
    "#                    output_folder='./inputData/',\n",
    "#                    max_len=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data parameters\n",
    "# data_folder = '../a-PyTorch-Tutorial-to-Image-Captioning/inputData/' \n",
    "data_folder = './inputData/'  # folder with data files saved by create_input_files.py\n",
    "data_name = 'coco_5_cap_per_img_5_min_word_freq'  # base name shared by data files\n",
    "\n",
    "# Read word map\n",
    "word_map_file = os.path.join(data_folder, 'WORDMAP_' + data_name + '.json')\n",
    "with open(word_map_file, 'r') as j:\n",
    "    word_map = json.load(j)\n",
    "benchmark = False # if GPU is rtx2080, else True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.  NIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "embed_encoder_dim = 512  # dimension of word embeddings\n",
    "hidden_dim = 512  # dimension of decoder RNN\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 vgg16 and NIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = Encoder()\n",
    "\n",
    "# decoder = DecoderWithoutAttention(image_output_dim = encoder.output_dim, hidden_dim = hidden_dim, \n",
    "#                                   vocab_size =len(word_map), embed_encoder_dim=embed_encoder_dim, \n",
    "#                                   device = device)\n",
    "# # checkpoint = 'BEST_checkpoint_vgg16_NIC_coco_5_cap_per_img_5_min_word_freq.pth.tar'\n",
    "# checkpoint = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# backprop_deep(encoder, decoder, data_folder, data_name, word_map, \n",
    "#               epochs = 120, decoder_lr = 4e-4 , checkpoint = checkpoint, device = device, benchmark = benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 densenet161 and NIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = Encoder('densenet161')\n",
    "\n",
    "# decoder = DecoderWithoutAttention(image_output_dim = encoder.output_dim,hidden_dim = hidden_dim, vocab_size =len(word_map),\n",
    "#                                       embed_encoder_dim=embed_encoder_dim, device = device)\n",
    "# checkpoint = 'BEST_checkpoint_densenet161_NIC_coco_5_cap_per_img_5_min_word_freq.pth.tar'\n",
    "# # checkpoint = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# backprop_deep(encoder, decoder, data_folder, data_name, word_map, \n",
    "#               epochs = 120, decoder_lr = 4e-4 , checkpoint = checkpoint, device = device, benchmark = benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3  resnet101 and NIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = Encoder('resnet101')\n",
    "\n",
    "# decoder = DecoderWithoutAttention(image_output_dim = encoder.output_dim,hidden_dim = hidden_dim, vocab_size =len(word_map),\n",
    "#                                       embed_encoder_dim=embed_encoder_dim, device = device)\n",
    "# checkpoint = 'BEST_checkpoint_resnet101_NIC_coco_5_cap_per_img_5_min_word_freq.pth.tar'\n",
    "# # checkpoint = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# backprop_deep(encoder, decoder, data_folder, data_name, word_map, \n",
    "#               epochs = 120, decoder_lr = 4e-4 , checkpoint = checkpoint, device = device, benchmark = benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. NICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "emb_dim = 512  # dimension of word embeddings\n",
    "attention_dim = 512  # dimension of attention linear layers\n",
    "hidden_dim = 512  # dimension of decoder RNN\n",
    "dropout = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 vgg16 and NICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = Encoder()\n",
    "\n",
    "# decoder = DecoderWithAttention(attention_dim=attention_dim, embed_dim=emb_dim, hidden_dim=hidden_dim,\n",
    "#                                    vocab_size=len(word_map), encoder_dim = encoder.output_dim, \n",
    "#                                    dropout=dropout, device = device)\n",
    "# # checkpoint = 'BEST_checkpoint_vgg16_NICA_coco_5_cap_per_img_5_min_word_freq.pth.tar'\n",
    "# checkpoint = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# backprop_deep(encoder, decoder, data_folder, data_name, word_map, \n",
    "#               epochs = 120, decoder_lr = 4e-4 , checkpoint = checkpoint, device = device, benchmark = benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 densenet161 and NICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder('densenet161')\n",
    "\n",
    "decoder = DecoderWithAttention(attention_dim=attention_dim, embed_dim=emb_dim, hidden_dim=hidden_dim,\n",
    "                                   vocab_size=len(word_map), encoder_dim = encoder.output_dim, \n",
    "                                   dropout=dropout, device = device)\n",
    "checkpoint = None\n",
    "checkpoint = 'BEST_checkpoint_densenet161_NICA_coco_5_cap_per_img_5_min_word_freq.pth.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/17702]\tBatch Time 0.864 (0.864)\tData Load Time 0.406 (0.406)\tLoss 3.4182 (3.4182)\tTop-5 Accuracy 72.145 (72.145)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-6cb7eec87ca3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m backprop_deep(encoder, decoder, data_folder, data_name, word_map, \n\u001b[0;32m----> 2\u001b[0;31m               epochs = 120, decoder_lr = 4e-4 , checkpoint = checkpoint, device = device, benchmark = benchmark)\n\u001b[0m",
      "\u001b[0;32m~/ECE285_ML/project/Image_Captioning/solver.py\u001b[0m in \u001b[0;36mbackprop_deep\u001b[0;34m(encoder, decoder, data_folder, data_name, word_map, epochs, decoder_lr, checkpoint, device, benchmark)\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgradients\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mclipped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mgrad_clip\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mclip\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m     \"\"\"\n\u001b[0m\u001b[1;32m    331\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ECE285_ML/project/Image_Captioning/solver.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, encoder, decoder, criterion, decoder_optimizer, epoch, device, grad_clip, print_freq, alpha_c)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaps_sorted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaplens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_model\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'NICA'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaps_sorted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaplens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ECE285_ML/project/Image_Captioning/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, encoder_out, encoded_captions, caption_lengths)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;31m# We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;31m# So, decoding lengths are actual lengths - 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mdecode_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcaption_lengths\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;31m# Create tensors to hold word predicion scores and alphas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "backprop_deep(encoder, decoder, data_folder, data_name, word_map, \n",
    "              epochs = 120, decoder_lr = 4e-4 , checkpoint = checkpoint, device = device, benchmark = benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3  resnet101 and NICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = Encoder('resnet101')\n",
    "\n",
    "# decoder = DecoderWithAttention(attention_dim=attention_dim, embed_dim=emb_dim, hidden_dim=hidden_dim,\n",
    "#                                    vocab_size=len(word_map), encoder_dim = encoder.output_dim, \n",
    "#                                    dropout=dropout, device = device)\n",
    "# checkpoint = None\n",
    "# # checkpoint = 'pre_train_BEST_checkpoint_coco_5_cap_per_img_5_min_word_freq.pth.tar'\n",
    "# # 'BEST_checkpoint_resnet101_NICA_coco_5_cap_per_img_5_min_word_freq.pth.tar'\n",
    "# checkpoint = 'BEST_checkpoint_resnet101_NICA_coco_5_cap_per_img_5_min_word_freq.pth.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# backprop_deep(encoder, decoder, data_folder, data_name, word_map, \n",
    "#               epochs = 120, decoder_lr = 4e-4 , checkpoint = checkpoint, device = device, benchmark = benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
